{
  "name": "webcrawler",
  "implementation": "tools.impl.webcrawler_tool.WebCrawlerTool",
  "description": "Crawl websites and extract content with configurable depth (max 3 levels)",
  "version": "1.0.0",
  "enabled": true,
  "inputSchema": {
    "type": "object",
    "properties": {
      "action": {
        "type": "string",
        "description": "Action to perform",
        "enum": [
          "crawl_url",
          "extract_links",
          "extract_content",
          "extract_metadata",
          "crawl_sitemap",
          "check_robots_txt",
          "get_page_info"
        ]
      },
      "url": {
        "type": "string",
        "description": "URL to crawl or analyze"
      },
      "max_depth": {
        "type": "integer",
        "description": "Maximum crawl depth (0-3)",
        "default": 1,
        "minimum": 0,
        "maximum": 3
      },
      "max_pages": {
        "type": "integer",
        "description": "Maximum number of pages to crawl",
        "default": 10,
        "minimum": 1,
        "maximum": 100
      },
      "follow_external": {
        "type": "boolean",
        "description": "Follow external links",
        "default": false
      },
      "respect_robots": {
        "type": "boolean",
        "description": "Respect robots.txt rules",
        "default": true
      },
      "extract_images": {
        "type": "boolean",
        "description": "Extract image URLs",
        "default": true
      },
      "extract_text": {
        "type": "boolean",
        "description": "Extract text content",
        "default": true
      },
      "delay": {
        "type": "number",
        "description": "Delay between requests in seconds",
        "default": 1.0,
        "minimum": 0.5,
        "maximum": 5.0
      },
      "timeout": {
        "type": "integer",
        "description": "Request timeout in seconds",
        "default": 10,
        "minimum": 5,
        "maximum": 30
      }
    },
    "required": ["action", "url"]
  },
  "metadata": {
    "author": "Ashutosh Sinha",
    "category": "Web Scraping",
    "tags": ["crawler", "scraper", "web", "html", "content extraction", "sitemap"],
    "rateLimit": 60,
    "cacheTTL": 1800,
    "note": "Maximum crawl depth is 3 levels. Respects robots.txt by default."
  }
}
